---
title: "Sentimen Analisis Joe Biden"
author: "Kelompok_6"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Import Library
```{r rlib}
library(tm) #data cleaning (corpus)
library(twitteR) #akses twitter APIs
library(rtweet) #collect and organize twitter data
library(shiny) #shiny
library(syuzhet) #baca fungsi get_nrc
library(wordcloud) #wordcloud
library(vroom) #load dataset
library(here) #menyimpan dataset
library(dplyr) #manipulasi data frame
library(ggplot2) #visualisasi data (barplot, grafik)
library(RColorBrewer) #pengaturan warna
library(RTextTools) #buat naive bayes

# Key auth Twitter API
  consumer.api_key <- "2G3RGOD52f87ufAsZy4devgJE"
  consumer.api_secret_key <- "yFRi2Wyc06OXCjs2I2dr0DpVMnDu7WYpaA0o6xIlmatZVkoxAD"
  access.token <- "1112575957070577664-hgcn1JZCvlXzAPW5OOEfxqzLN9duNF"
  access.token_secret <- "ryCbenRhXbWxa0RvjlOQvg7WChtIfKFYg0MOdiIXyooIG"
  
# Start authentication with OAuth
setup_twitter_oauth(consumer.api_key, consumer.api_secret_key, access.token, access.token_secret)
```


```{r}
tweets = searchTwitter('Joe Biden', 
                               n = 1000,
                               lang = "en",
                               retryOnRateLimit = 10e5)
text <- do.call("rbind", lapply(tweets, as.data.frame))

write.csv(text, file = 'dataMentah.csv')
```

```{r}
d <- read.csv("dataMentah.csv")
kata <- d$text
reviewC <- Corpus(VectorSource(kata))
#remove URL
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
reviewclean <- tm_map(reviewC, removeURL)
#remove New Line
removeNL <- function(y) gsub("\n", " ", y)
reviewclean <- tm_map(reviewclean, removeNL)
#remove koma
replacecomma <- function(y) gsub(",", "", y)
reviewclean <- tm_map(reviewclean, replacecomma)
#remove retweet
removeRT <- function(y) gsub("RT ", "", y)
reviewclean <- tm_map(reviewclean, removeRT)
#remove titik dua
removetitik2 <- function(y) gsub(":", "", y)
reviewclean <- tm_map(reviewclean, removetitik2)
#remove titik koma
removetitikkoma <- function(y) gsub(";", " ", y)
reviewclean <- tm_map(reviewclean, removetitikkoma)
#remove titik3
removetitik3 <- function(y) gsub("p…", "", y)
reviewclean <- tm_map(reviewclean, removetitik3)
#remove &amp
removeamp <- function(y) gsub("&amp;", "", y)
reviewclean <- tm_map(reviewclean, removeamp)
#remove Mention
removeUN <- function(z) gsub("@\\w+", "", z)
reviewclean <- tm_map(reviewclean, removeUN)

removesym <- function(y) gsub("ð", "", y)
reviewclean <- tm_map(reviewclean, removesym)

#remove Emoji
removeEmoji <- function(z) gsub("[^\x01-\x7F]", "", z)
reviewclean <- tm_map(reviewclean, removeEmoji)
#remove Number
removeNum <- function(z) gsub("[0-9]+", "", z)
reviewclean <- tm_map(reviewclean, removeNum)
#remove space dll
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)

reviewclean <- tm_map(reviewclean,remove.all)
reviewclean <- tm_map(reviewclean, removePunctuation) #tanda baca
reviewclean <- tm_map(reviewclean, tolower) #mengubah huruf kecil

#hapus data yang kosong
try.error = function(x)
{
  # create missing value
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error=function(e) e)
  # if not an error
  if (!inherits(try_error, "error"))
    y = tolower(x)
  # result
  return(y)
}

# lower case using try and error with sapply 
reviewclean = sapply(reviewclean, try.error)

# remove NAs in some_txt
reviewclean = reviewclean[!is.na(reviewclean)]
names(reviewclean) = NULL

dataframe<-data.frame(text=unlist(sapply(reviewclean, `[`)), stringsAsFactors=F)
write.csv(dataframe,file = "data_clean.csv")
View(dataframe)

try_clean <- read.csv('data_clean.csv')

#skoring
kata.positif <- scan("positive-words.txt",what="character",comment.char=";")
kata.negatif <- scan("negative-words.txt",what="character",comment.char=";")
score.sentiment = function(sentence, positif, negatif,
                           .progress='none')
{
  require(plyr)
  require(stringr)
  scores = laply(sentence, function(kalimat, positif,
                                    negatif) {
    kalimat = gsub('[[:punct:]]', '', kalimat)
    kalimat = gsub('[[:cntrl:]]', '', kalimat)
    kalimat = gsub('\\d+', '', kalimat)
    kalimat = tolower(kalimat)
    list.kata = str_split(kalimat, '\\s+')
    kata2 = unlist(list.kata)
    positif.matches = match(kata2, kata.positif)
    negatif.matches = match(kata2, kata.negatif)
    positif.matches = !is.na(positif.matches)
    negatif.matches = !is.na(negatif.matches)
    score = sum(positif.matches) - (sum(negatif.matches))
    return(score)
  }, kata.positif, kata.negatif, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentence)
  return(scores.df)}

hasil = score.sentiment(try_clean$text, kata.positif, kata.negatif)

#konversi score ke sentiment
hasil$klasifikasi<- ifelse(hasil$score<0, "Negatif",ifelse(hasil$score==0,"Netral","Positif"))
hasil$klasifikasi
View(hasil)

#menukar urtan baris
data <- hasil[c(3,1,2)] #ubah urutan kolom
View(data)
write.csv(data, file = "dataLabel.csv")
```

```{r naive bayes}
library(e1071) #library yang terdapat sebuah algoritma naivebayes
library(caret) #library yang terdapat sebuah algoritma naivebayes

d<-read.csv("data_clean.csv",stringsAsFactors = FALSE) #membaca file csv yang sudah di cleaning data

review <-as.character(d$text) #set variabel cloumn text menjadi char
get_nrc_sentiment('happy')
get_nrc_sentiment('excitement')
#deklarasi var s utnuk memanggil sentimen dictionary untuk menghitung presentasi dari beberapa emotion dan mengubahnya ke dalam text file
s <- get_nrc_sentiment(review, cl = NULL, language = "english", lowercase = TRUE)

review_combine<-cbind(d$text,s) #klasifikasi data
par(mar=rep(3,4))
a<- barplot(colSums(s),col=rainbow(10), xlab ='emotion', ylab='count',main='Sentiment Analysis')
barplt <- a
```


```{r}
require(corpus)

data.frame <- read.csv("dataLabel.csv",stringsAsFactors = F)
data.frame$klasifikasi <- factor(data.frame$klasifikasi)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])

#fungsinya untuk membersihkan data data yang tidak dibutuhkan 

corpus.clean<-corpus %>%
  tm_map(content_transformer(tolower)) %>% #digunakan untuk mengubah huruf besar dari string menjadi string huruf kecil
  tm_map(removePunctuation)%>% #menghapus tanda baca
  tm_map(removeNumbers)%>% #menghapus nomor
  tm_map(removeWords,stopwords(kind="en"))%>% #menghapus stopwords
  tm_map(stripWhitespace) 
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])

df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]   

dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]

corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]

dim(dtm.train)

fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)

dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)

dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)

#Boolan Naive Bayes
convert_count <- function(x){
    y<-ifelse(x>0,1,0)
    y<-factor(y,levels=c(0,1),labels=c("no","yes"))
    y
}

#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)

#Use the NB classifier we built to make predictions on the test set
pred <- predict(classifier, testNB)

#Create a truth table by tabulating the predicted class labels with the actual predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$klasifikasi)
NB_table

#confussion Matrix
conf.matNB <- confusionMatrix(pred, df.test$klasifikasi)
conf.matNB

library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=230,random.order=F,colors=brewer.pal(8,"Dark2"))

kalimat2<-read.csv("data_clean.csv",header=TRUE)
```

```{r Freq}
  data1 = read.csv("dataLabel.csv")
  corpus = Corpus(VectorSource(data1$text))
      corpus <- tm_map(corpus, removeWords,"nya")
      corpus <- tm_map(corpus, removeWords,"aja")
      corpus <- tm_map(corpus, removeWords,"gak")
      corpus <- tm_map(corpus, removeWords,"ðÿ")
      corpus <- tm_map(corpus, removeWords,"amp")
      corpus <- tm_map(corpus, removeWords,"ari")
      corpus <- tm_map(corpus, removeWords,"seniri")
  dtm <- TermDocumentMatrix(corpus)
  m <- as.matrix(dtm)
  v <- sort(rowSums(m),decreasing=TRUE)
  d <- data.frame(word = names(v),freq=v)
  barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
        main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
```

**User Interface**
```{r}
#shiny
#membuka file csv
dataLabel<- read.csv("datalabel.csv")
dataKotor <- read.csv("dataMentah.csv")

#mengatur tampilan web
ui <- fluidPage(
   titlePanel("Analisis Sentimen Pada Data Twitter Mengenai Joe Biden Dengan Metode Naive Bayes"), #judul
    # Show a plot of the generated distribution
   mainPanel(#tab
    #plot output : untuk scatterplot
            tabsetPanel(type = "tabs",
                         tabPanel("Term Document Matrix and Statistic", verbatimTextOutput("result")),
                        #tab data kotor dan hasil sentiment
                        tabPanel("List Kotor", DT::dataTableOutput('tbl1')),
                        tabPanel("List sentiment", DT::dataTableOutput('tbl2')),
                        #tab scatterplot/grafik
                        tabPanel("Histogram", plotOutput("scatterplot")), 
                        tabPanel("Frequency", plotOutput("freqplot")), 
                        # tab wordcloud
                        tabPanel("Wordcloud", plotOutput("Wordcloud")),
            )
   )
    
)
```

**Server**
```{r global}
#tempat data akan dianalisis dan diproses, hasilnya ditampilkan/diplotkan pada bagian mainpanel() ui
server <- function(input, output) {
  #output Data
  output$result <-renderPrint({
      classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
      NB_table=table("Prediction"= pred, "Actual" = df.test$klasifikasi)
      conf.matNB <- confusionMatrix(pred, df.test$klasifikasi)
      conf.matNB
  })
  #data ditampilkan dalam beberapa halaman
  output$tbl1 = DT::renderDataTable({
        DT::datatable(dataKotor, options = list(lengthChange = FALSE))
  })

  output$tbl2 = DT::renderDataTable({
        DT::datatable(dataLabel, options = list(lengthChange = FALSE))
  })
  
  #barplot
  output$scatterplot <- renderPlot({wadas_dataset<-read.csv("data_clean.csv",stringsAsFactors = FALSE)
  review <-as.character(wadas_dataset$text)
  get_nrc_sentiment('happy')
  get_nrc_sentiment('excitement')
  s<-get_nrc_sentiment(review)
  review_combine<-cbind(wadas_dataset$text,s)
  par(mar=rep(3,4))
  barplot(colSums(s),col=rainbow(10),ylab='count',main='Sentiment Analysis')
  }, height=400)
  
  #wordcloud
  output$Wordcloud <- renderPlot({
    set.seed(20)
    df<-df[sample(nrow(df)),]
    df<-df[sample(nrow(df)),]
    glimpse(df)
    df$X=as.factor(df$X)
    corpus<-Corpus(VectorSource(df$text))
    corpus
    inspect(corpus[1:10])
    
    output$freqplot <- renderPlot({data1 = read.csv("dataLabel.csv")
    corpus = Corpus(VectorSource(data1$text))
    corpus <- tm_map(corpus, removeWords,"nya")
    corpus <- tm_map(corpus, removeWords,"aja")
    corpus <- tm_map(corpus, removeWords,"gak")
    corpus <- tm_map(corpus, removeWords,"ðÿ")
    
    dtm2 <- TermDocumentMatrix(corpus)
    m <- as.matrix(dtm2)
    v <- sort(rowSums(m),decreasing=TRUE)
    d <- data.frame(word = names(v),freq=v)
    barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
        main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
          }, height=400)
    
    #membersihkan data data yang tidak dibutuhkan
    corpus.clean<-corpus %>%
      tm_map(content_transformer(tolower)) %>%
      tm_map(removePunctuation) %>%
      tm_map(removeNumbers) %>%
      tm_map(removeWords,stopwords(kind="en")) %>%
      tm_map(stripWhitespace)
      dtm<-DocumentTermMatrix(corpus.clean)
      inspect(dtm[1:10,1:20])
      df.train<-df[1:589,]
      df.test<-df[590:1177,]
      dtm.train<-dtm[1:589,]
      dtm.test<-dtm[590:1000,]
      corpus.clean.train<-corpus.clean[1:589]
      corpus.clean.test<-corpus.clean[590:1000]
      dim(dtm.train)
      fivefreq<-findFreqTerms(dtm.train,5)
      length(fivefreq)
      dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
      dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
      dim(dtm.test.nb)
      convert_count <- function(x){
        y<-ifelse(x>0,1,0)
        y<-factor(y,levels=c(0,1),labels=c("no","yes"))
        y
      }
      
      trainNB<-apply(dtm.train.nb,2,convert_count)
      testNB<-apply(dtm.test.nb,1,convert_count)
      classifier<-naiveBayes(trainNB,df.train$X,laplace = 1)
      
 library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
  })
  
}
shinyApp(ui = ui, server = server)
```


